import nltk 
from nltk.tokenize import word_tokenize 
from nltk.stem import PorterStemmer, WordNetLemmatizer 
from nltk.corpus import stopwords 

def preprocess_text(text): 
 
words = word_tokenize(text) 
 
words = [word.lower() for word in words if word.isalpha()] 

stop_words = set(stopwords.words('english')) 
words = [word for word in words if word not in stop_words] 
return words 
def stem_text(words): 

stemmer = PorterStemmer() 

stemmed_words = [stemmer.stem(word) for word in words] 
return stemmed_words 
def lemmatize_text(words): 

lemmatizer = WordNetLemmatizer() 

lemmatized_words = [lemmatizer.lemmatize(word) for word in words] 
return lemmatized_words 
if __name__ == "__main__": 

sample_text = "The quick brown foxes are jumping over the lazy dogs." 
 
processed_words = preprocess_text(sample_text) 
 
stemmed_words = stem_text(processed_words) 

lemmatized_words = lemmatize_text(processed_words) 
 
print("Original Words:", processed_words) 
print("Stemmed Words:", stemmed_words) 
print("Lemmatized Words:", lemmatized_words) 
